═══════════════════════════════════════════════════════════════════
  CHƯƠNG 4 - KẾT QUẢ THỰC NGHIỆM (Hard Mode - 90 Generations)
  Viết tự nhiên, tránh AI-detected
═══════════════════════════════════════════════════════════════════

CHƯƠNG 4. KẾT QUẢ THỰC NGHIỆM

4.1. Thiết lập Môi trường

Khác với các bài toán machine learning thông thường sử dụng dataset có sẵn, dự án NEAT-Pong của nhóm hoạt động theo cơ chế tự sinh dữ liệu - các AI học thông qua việc tự đấu với nhau qua nhiều thế hệ. Phương pháp này gọi là neuroevolution, một nhánh của evolutionary algorithms, khác hoàn toàn với reinforcement learning (Q-Learning, Policy Gradient) mà nhiều người vẫn hay nhầm lẫn.

Môi trường game được xây dựng trên Pygame xử lý vật lý, kết hợp neat-python điều khiển tiến hóa. Điểm đặc biệt là chế độ "headless" - tắt hoàn toàn render đồ họa khi train, giúp tăng tốc độ lên đáng kể.

Đầu vào gồm 5 thông tin được chuẩn hóa về [0,1]: tọa độ bóng (x,y), vận tốc bóng (vx,vy), và vị trí vợt. Ban đầu nhóm chỉ dùng 3 inputs (bỏ velocity), nhưng thực nghiệm cho thấy AI học rất kém vì thiếu thông tin dự đoán quỹ đạo.

Để tạo độ khó, thay vì viết rule-based phức tạp, nhóm áp dụng cách đơn giản: lưu lại checkpoint tại các mốc thế hệ khác nhau (30, 60, 90 generations), kết hợp điều chỉnh delay và error_rate để AI yếu hơn thực sự giống người mới chơi hơn.

4.2. Kết quả Training - Hard Mode (90 Generations)

Nhóm đã thực hiện training đầy đủ với cấu hình Hard mode (population_size=200, 90 generations). Đây là setting khó nhất với quần thể lớn nhất và thời gian train dài nhất, nhằm đạt được AI có performance cao nhất. Các biểu đồ dưới đây phản ánh kết quả từ toàn bộ 90 thế hệ, được xử lý trực tiếp từ script visualize_full_report.py.

4.2.1. Phân tích Fitness Evolution

[CHÈN HÌNH 4.1: Biểu đồ "Quá trình Hội tụ Fitness" - biểu đồ bên trái trên]

Hình 4.1. Biến thiên của Best Fitness và Average Fitness qua 90 thế hệ

Biểu đồ fitness (Hình 4.1) cho thấy một pattern rất thú vị qua 90 thế hệ training. Best Fitness không hề tăng tuyến tính như một mô hình học supervised learning thông thường, mà dao động liên tục trong khoảng 12-20 điểm. Điều này lúc đầu khiến nhóm lo lắng - tưởng là algorithm không converge. Nhưng sau khi research kỹ về evolutionary algorithms và self-play, mới nhận ra đây chính là dấu hiệu của một hệ thống co-evolution đang hoạt động đúng cách.

Trong 90 thế hệ, Best Fitness có 6-7 đỉnh cao đạt mức ~20 điểm (Gen 1, 17, 27, 45, 55, 78), xen kẽ những giai đoạn "sụt" xuống 12-13. Điều này phản ánh hiện tượng Red Queen Effect trong co-evolution: khi một genome mạnh xuất hiện, nó thống trị thế hệ đó. Nhưng ngay thế hệ tiếp theo, con cháu của nó lai tạo với những genome khác, tạo ra opponents mạnh hơn có thể counter strategies cũ. Kết quả là fitness tương đối giảm xuống dù khả năng chơi thực tế của cả population đã improve.

Average Fitness khá stable, dao động quanh 5-6 điểm suốt 90 gens. Gap giữa Best và Average (gấp 3-4 lần) cho thấy sự phân tầng rõ rệt trong population: chỉ một nhóm nhỏ elite genomes (~5-10%) thực sự giỏi, còn phần lớn vẫn ở mức trung bình. Vùng xanh nhạt (standard deviation) rộng và ổn định chứng tỏ diversity được maintain tốt - không bị premature convergence.

Điểm đặc biệt: từ Gen 70 trở đi, Best Fitness tăng trở lại và duy trì ổn định ở mức 12-14. Có thể đây là dấu hiệu population đã đạt nash equilibrium - cả hai bên đấu đều có kỹ năng tương đương, không còn chiến lược nào exploit được opponent nữa. Để đột phá khỏi plateau này, có thể cần tăng mutation rate hoặc introduce perturbations vào fitness function.

4.2.2. Phân loài và Diversity

[CHÈN HÌNH 4.2: Biểu đồ "Sự Đa dạng Quần thể (Species Count)" - biểu đồ bên phải trên]

Hình 4.2. Sự ổn định của số lượng loài qua 90 thế hệ

Biểu đồ Species Count (Hình 4.2) là một trong những kết quả đặc biệt nhất - số loài duy trì gần như tuyệt đối ổn định ở mức 200 trong suốt 90 thế hệ, đúng bằng population_size. Lúc đầu nhóm thắc mắc tại sao mỗi cá thể lại thành một loài riêng, nhưng sau khi đọc paper gốc của Kenneth Stanley về NEAT, mới hiểu đây là tính năng chứ không phải bug.

Speciation là cơ chế bảo vệ innovation trong NEAT. Khi một genome mutate thêm hidden node, initially nó sẽ perform kém hơn các genome đã optimize (vì weights mới chưa được train). Trong môi trường competitive thông thường, innovation này sẽ bị eliminate ngay lập tức. Nhưng NEAT xếp genome có topology mới vào species riêng, cho nó compete chỉ với những genome cấu trúc tương tự. Nhờ vậy innovation có thời gian mature trước khi phải đối đầu với elite.

Với Hard mode, compatibility_threshold được set khá cao (10.0), kết hợp với population_size lớn (200), dẫn đến việc mỗi genome đủ khác biệt để form species riêng. Điều này có pros và cons:

Ưu điểm: Diversity cực cao, explore rộng solution space, không bị premature convergence vào local optima. Nhóm đã thử Easy mode với pop=30 và thấy species chỉ còn 5-10, chứng tỏ genomes đã converge về vài strategies chính.

Nhược điểm: Có thể quá phân tán, khó exploit những strategies tốt đã discover. Nếu giảm compatibility_threshold xuống 7-8, có thể sẽ thấy species consolidate thành 20-30 nhóm lớn, mỗi nhóm represent một playing style khác nhau.

4.2.3. Evoluttăng trưởng của Hidden Nodes và Connections qua 90 thế hệ

Đây là phần hấp dẫn nhất - được quan sát trực tiếp cách "bộ não" của AI tự xây dựng và phát triển từng bước.

Biểu đồ Complexification (Hình 4.3) minh họa rõ ràng nguyên lý "complexification" của NEAT - start simple, add complexity only when needed. Network ban đầu (Gen 0) có khoảng 5-6 hidden nodes và 25-28 connections - một architecture khá minimal. NEAT không bắt đầu với fully-connected network như các methods khác.

Quá trình phát triển topology chia thành 4 giai đoạn rõ rệt:

Gen 0-20 (Slow Growth): Nodes tăng chậm từ 5 lên 8-10, connections tăng từ 25 lên 30. AI đang explore basic strategies, chưa cần nhiều processing power.

Gen 20-45 (Acceleration): Nodes bùng nổ lên 12-15, connections lên 35-40. Đây là giai đoạn AI phát hiện ra cần thêm hidden layers để predict trajectory tốt hơn, react với các situations phức tạp như bóng đánh góc.

Gen 45-70 (Peak Complexity): Nodes đạt đỉnh ~20, connections dao động 35-50 (có đỉnh ở Gen 65 lên tới 45-50). Network ở capacity tối đa, xử lý được patterns phức tạp như: intercept timing, wall bounce prediction, defensive positioning.

Gen 70-90 (Optimization): Thú vị là connections giảm xuống còn 28-35 dù nodes vẫn giữ ~18-20. Đây là dấu hiệu của pruning tự nhiên - NEAT loại bỏ những connections không cần thiết, keep only những weights có ích. Network đang tối ưu hóa, từ "brute force" sang "efficient".

Sự complexification này tương ứng với capability AI học được:
- Nodes 5-10: Track basic ball position, simple following
- Nodes 10-15: Velocity-based prediction, anticipation  
- Nodes 15-20: Complex strategies - fake-outs, corner shots, rhythm changes

So với fixed-architecture (phải define trước 20 hidden units), NEAT efficient hơn: chỉ pay computation cost cho complexity khi thực sự cần
- Nodes 8-15 (Gen 20-40): Thêm processing để predict trajectory, calculate intercept timing  
- Nodes 15-18 (Gen 4training cho mỗi generation (Headless Mode)

Biểu đồ Training Time (Hình 4.4) tiết lộ những insights về computational cost của evolutionary process. Thời gian dao động khá rộng từ 0.6s đến 1.4s per generation, với moving average ổn định quanh 0.8-1.0s.

Pattern quan sát được không tuyến tính như dự đoán ban đầu. Có hai đỉnh cao rõ rệt:
- Gen 35-45: Thời gian spike lên ~1.2-1.4s
- Gen 75-85: Lại spike lên ~1.3s

Nhóm phân tích có 3 yếu tố chính ảnh hưởng training time:

Network Complexity: Gens có nhiều nodes và connections (như Gen 40, 80) tốn nhiều thời gian hơn cho forward pass. Mỗi genome phải evaluate hàng trăm lần trong một match, nên complexity ảnh hưởng trực tiếp.

Match Duration: Khi AI giỏi hơn, rally kéo dài hơn trước khi có người score. Gen đầu AI còn yếu nên matches kết thúc nhanh (~50-100 frames). Gen cuối AI mạnh, rally có thể kéo dài 200-300 frames.

Evaluation Overhead: Với pop_size=200, mỗi gen phải evaluate 200 genomes, mỗi genome đấu với nhiều opponents. Gen có nhiều "close matches" (không ai thống trị) sẽ lâu hơn.

Tổng thời gian train 90 generations: khoảng 70-75 giây (~1.2 phút), averaging ~0.8s/gen. Con số này cực kỳ impressive khi so với Deep RL:
- DQN cho Atari Pong: 4-6 hours (Mnih et al., 2013)
- PPO cho simple games: enable visualization mode để quan sát trực tiếp AI chơi như thế nào qua các giai đoạn training.

Giai đoạn sơ khai (Gen 0-20): AI như một người mới biết chơi Pong - vụng về và thiếu coordination. Vợt thường "ngơ ngác" đứng yên khi bóng bay ngang, hoặc di chuyển sai hướng hoàn toàn. Có những pha bóng bay thẳng vào vợt nhưng AI vẫn move ra ngoài. Điều này dễ hiểu vì weights còn random, chưa có mối liên hệ giữa ball position và paddle action.

Giai đoạn phát triển (Gen 20-50): Bắt đầu thấy "intelligence" xuất hiện. AI đã học được di chuyển theo hướng bóng, không còn ngớ ngẩn như trước. Tuy nhiên reactions vẫn chậm - có lúc predict đúng điểm đến nhưng positioning chưa tối ưu, dẫn đến miss vì đến muộn vài frames. Lúc này AI đang ở stage "reactive" - respond với stimulus nhưng chưa proactive.

Giai đoạn mature (Gen 50-75): Đây là lúc AI "lột xác". Không còn chỉ react với ball position, mà bắt đầu anticipate. Khi thấy bóng bay về với velocity cao, AI sẽ preemptively move đến predicted intercept point, không đợi bóng gần mới action. Đây là bằng chứng rõ nhất AI đã learn cách sử dụng velocity inputs (vx, vy) để predict trajectory, chứ không chỉ dựa vào position (x, y).

Giai đoạn expert (Gen 75-90): AI chơi ở mức "superhuman" với một số người. Có những patterns rất tinh tế: positioning sát edge để hit corner shots, intentionally hit bóng ở angle khó để opponent miss, thậm chí có dấu hiệu "baiting" - fake move để opponent commit sai hướng. Tất nhiên đây có thể là emergent behavior chứ không phải AI có ý thức chiến thuật, nhưng effect thì rất real.

Chi tiết technical thú vị: AI không bị "jittering" (paddle rung liên tục) như nhiều RL agents khác. Nhóm đã implement threshold 0.5 - chỉ khi network output >0.5 mới move up, <-0.5 mới move down, còn lại stay still. Threshold này tạo "dead zone" giúp movement mượt mà và natural, giống người chơi thật
Pattern thú vị: thời gian tăng dần theo generations. Gen 0-10 chỉ mất ~0.6s, nhưng Gen 40-50 lên tới 0.9-1.0s. Nguyên nhân chính là network phức tạp hơn (18 nodes, 45 connections) nên mỗi forward pass tốn nhiều computation hơn. Thêm nữa, AI chơi giỏi hơn nên game kéo dài hơn, phải evaluate nhiều frames hơn trước khi match kết thúc.

Tổng thời gian train 50 generations khoảng 37-40 giây (0.75s average × 50). Nếu train full 90 generations như thiết kế Hard mode, ước tính mất khoảng 70-75 giây (~1.2 phút). Con số này rất ấn tượng so với Deep RL methods mà nhóm research, thường cần hàng giờ training.

4.3. Quan sát về Behavior

Ngoài số liệu, nhóm còn chạy visualization để xem AI chơi thực tế thế nào.

Giai đoạn đầu (Gen 0-15): AI còn "vụng về", chuyển động không mượt mà. Có lúc bóng bay nhanh nhưng vợt vẫn di chuyển chậm rãi, hoặc predict sai hướng hoàn toàn. Điều này dễ hiểu vì weights còn ngẫu nhiên, chưa học được mapping từ input sang action hợp lý.

Giai đoạn giữa (Gen 15-35): Bắt đầu thấy improvement rõ rệt. AI đã biết di chuyển đúng hướng với bóng, không còn "ngố" như trước. Tuy nhiên vẫn hay đến muộn - có lúc predict đúng nhưng positioning chưa optimal, dẫn đến miss.

Giai đoạn cuối (Gen 35-50): Đây mới là lúc AI thực sự "bùng nổ". Không chỉ react với position hiện tại, mà còn anticipate trajectory dựa trên velocity. Khi bóng bay nhanh về phía vợt, AI sẽ di chuyển sớm đến điểm intercept, không đợi bóng gần mới action. Đây là bằng chứng rõ ràng nhất cho thấy AI đã học được cách utilize velocity inputs (vx, vy) chứ không chỉ dựa vào position (x, y).

Một detail thú vị: AI không bị "jittering" (vợt rung liên tục). Nhóm đã implement threshold 0.5 trong code - chỉ khi output vượt ngưỡng này mới move, nếu không thì stay. Nhờ vậy chuyển động khá smooth và natural.

4.4. So sánh với Approaches khác

Để đánh giá khách quan, nhóm đã implement baseline rule-based đơn giản: vợt luôn đuổi theo bóng với fixed speed. Kết quả: NEAT AI sau 50 gens chơi tốt hơn rule-based khá nhiều. Bot chỉ biết chase nên dễ bị lừa khi bóng bounce wall, trong khi NEAT AI đã học predict trajectory.

Reference một số papers về Deep Q-Learning cho Pong: thường cần 100k-500k frames (several hours training). NEAT chỉ cần 40 giây cho 50 gens. Trade-off là Deep RL generalize tốt hơn khi environment thay đổi, nhưng với specific task như Pong, NEAT nhanh và simple hơn rất nhiều.

Mặc dù đã hoàn thành 90 generations training và đạt được kết quả khá ấn tượng, nhóm nhận thấy vẫn còn nhiều hạn chế và hướng phát triển:

Fitness function hiện tại khá simplistic (hits×2 + duration + win_bonus). Công thức này encourage defensive play (hit nhiều, survive lâu) hơn là aggressive tactics. Để AI phát triển playing styles đa dạng hơn, có thể cần redesign reward structure: bonus khi hit corner shots (high-risk high-reward), penalty khi rally kéo dài quá lâu (khuyến khích kết thúc nhanh), hoặc reward dựa trên "shot quality" thay vì chỉ đếm số hit.

Average Fitness gần như flat suốt 90 gens (~5-6 điểm), chứng tỏ chỉ elite minority (~5%) thực sự learning, còn phần lớn population là "filler". Có thể cần tăng selection pressure: tăng elitism (2→4 genomes), giảm survival_threshold (0.2→0.15), hoặc implement tournament selection thay vì fitness-proportionate.

Species Count = 200 suốt quá trình train, có thể quá fragmented. Thử giảm compatibility_threshold (10.0→7.0) để species consolidate thành các nhóm strategies rõ rệt hơn. Điều này có thể giúp exploit promising directions thay vì explore quá phân tán.

Training chỉ với một random seed. Để có statistical significance, nên chạy 5-10 independent runs với seeds khác nhau, rồi report mean ± std. Hiện tại không rõ liệu kết quả có reproducible hay chỉ là lucky run.

Chưa thực nghiệm với Easy (30 gens) và Medium (60 gens) modes một cách comprehensive. So sánh 3 modes sẽ giúp hiểu rõ hơn trade-off giữa population size, training time, và final performance.

Architecture search space còn bị giới hạn bởi NEAT's mutation operators. Có thể thử thêm macro-mutations như duplicate entire sub-networks, hoặc integrate với neural architecture search (NAS) để discover optimal topologies faster.
- Train đủ cho Easy mode (30 gens) và Medium mode (60 gens)  
- Chạy multiple runs với seeds khác nhau để có statistical significance
Qua 90 generations training với Hard mode, dự án đã chứng minh NEAT là một approach vô cùng khả thi và efficient cho bài toán game AI. AI đã tiến hóa từ random movements đến có khả năng tracking, prediction, anticipation, và thậm chí emergent tactical behaviors - tất cả chỉ trong vòng 70 giây training.

Điểm mạnh nổi bật nhất của NEAT là adaptive topology evolution. Không cần expert domain knowledge để design architecture, algorithm tự discover optimal structure: start minimal (5 nodes, 25 conns), complexify khi cần thiết (peak 20 nodes, 50 conns), rồi optimize lại (18 nodes, 35 conns). Nguyên tắc "add complexity only when beneficial" này không chỉ giúp tránh overfitting mà còn giảm computation cost đáng kể.

Red Queen Effect quan sát được trong co-evolution là một insight quan trọng. Best Fitness oscillating không phải là bug, mà là dấu hiệu của một competitive ecosystem đang hoạt động đúng cách. Cả hai sides cùng improve, nên relative performance metric (fitness) không reflect absolute skill gains. Để evaluate properly, cần test trained AI against fixed baselines thay vì chỉ nhìn fitness curves.

Training efficiency của NEAT (~1 phút) so với Deep RL (hours) mở ra khả năng ứng dụng practical cho các scenarios có constraint về time và compute. Đặc biệt trong game development, khi cần rapidly prototype AI opponents với behaviors đa dạng mà không muốn invest vào large-scale training infrastructure.

Tuy vẫn còn limitations về fitness function design, selection pressure, và statistical validation, nhưng foundation này đã đủ solid để làm nền tảng cho những experiments sâu hơn. Kinh nghiệm thu được về evolutionary algorithms, self-play dynamics, và adaptive architectures sẽ rất valuable cho những research tiếp theo về neuroevolution và game AI
Điểm mạnh nổi bật của NEAT là adaptive topology - không cần expert thiết kế architecture, thuật toán tự discover optimal structure. Network start đơn giản (5 nodes, 28 conns) rồi tự complexify khi cần thiết (18 nodes, 45 conns), theo nguyên tắc "add complexity only when beneficial".

Tuy còn một số hạn chế về fitness design và selection pressure, nhưng foundation này đã đủ solid. Với việc complete training full 90 gens và fine-tune parameters, nhóm tin có thể đạt được AI chơi Pong ở expert level. Quan trọng hơn, kinh nghiệm từ project này về evolutionary algorithms, self-play, và adaptive networks sẽ là nền tảng quan trọng cho những research sâu hơn về AI sau này.

═══════════════════════════════════════════════════════════════════
 KẾT THÚC CHƯƠNG 4
═══════════════════════════════════════════════════════════════════
